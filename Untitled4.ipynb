{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjgub1vbyPYpxAXWC+AKw+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "anOH1yiz9INd"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = fetch_20newsgroups()"
      ],
      "metadata": {
        "id": "ni2QFMNC9eEm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "H1jHdMzG9rDs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = text_data.data[0:4]\n",
        "# rw_text"
      ],
      "metadata": {
        "id": "KCBvZoGV-nQU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text1= []\n",
        "def to_lower_case(data):\n",
        "  for words in raw_text:\n",
        "    clean_text1.append(str.lower(words))\n",
        "    "
      ],
      "metadata": {
        "id": "ghzZyFaF_uNn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_lower_case(raw_text)"
      ],
      "metadata": {
        "id": "spPLWqb3AdsC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_text1\n"
      ],
      "metadata": {
        "id": "gLx66L8zCDYK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stage 2\n",
        "Tokenisation\n"
      ],
      "metadata": {
        "id": "Ez8CTkRiCgA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clean_text2 = []\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6sOakV1CS0v",
        "outputId": "2c60e75a-cff6-4754-eba7-f00c13c905da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_token=[]\n",
        "for sent in clean_text1:\n",
        "  sent= sent_tokenize(sent)\n",
        "  sent_token.append(sent)\n"
      ],
      "metadata": {
        "id": "Ug6pM7ELDQBo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text2 = [ word_tokenize(i) for i in clean_text1]"
      ],
      "metadata": {
        "id": "nmqq0E1AaSIN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_text2"
      ],
      "metadata": {
        "id": "syVcweMEa9-p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "clean_text3= []\n",
        "for words in clean_text2:\n",
        "  clean= []\n",
        "  for w in words:\n",
        "    res= re.sub(r'[^\\w\\s]',\"\",w)\n",
        "    if res !=\"\":\n",
        "      clean.append(res)\n",
        "    clean_text3.append(clean)"
      ],
      "metadata": {
        "id": "1mAsM5l_bEpo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_text3"
      ],
      "metadata": {
        "id": "eff3JGxjc7mI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di_xMK94dCqi",
        "outputId": "a1c09def-8aa8-46d7-f656-e11fe2e4245f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "2wPI7BundPO4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text4= []\n",
        "for words in clean_text3:\n",
        "  w= []\n",
        "  for word in words:\n",
        "   if not word in stopwords.words('english'):\n",
        "    w.append(word)\n",
        "  clean_text4.append(w)  "
      ],
      "metadata": {
        "id": "GzEouRUIdVuK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_text4"
      ],
      "metadata": {
        "id": "jLOd2mhUeCKv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jPEY-ND2eRJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sTAGE 5  STEMMING\n"
      ],
      "metadata": {
        "id": "-hKp8CDJeTnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "gOA4x8mjeY8W"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port = PorterStemmer()"
      ],
      "metadata": {
        "id": "7AlvBvWAef5I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [port.stem(i) for i in [\"reading\", \"washing\", \"driving\"]]\n",
        "# a"
      ],
      "metadata": {
        "id": "66c5OXSkeixY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text5= []\n",
        "for words in clean_text4:\n",
        "  w=[]\n",
        "  for word in words:\n",
        "    w.append(word)\n",
        "  clean_text5.append(w)\n"
      ],
      "metadata": {
        "id": "079B783Ge5cn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_text5"
      ],
      "metadata": {
        "id": "mPypTBTrgzI_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "NzmCfIX9hEyn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnet = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "0-t5ZkG4hqpq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gAtQf7JhvC4",
        "outputId": "c59eb720-6724-418b-ba9c-4d5332174505"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem= []\n",
        "for words in clean_text4:\n",
        "  w= []\n",
        "  for word in words:\n",
        "    w.append(wnet.lemmatize(word))\n",
        "  lem.append(w)\n"
      ],
      "metadata": {
        "id": "a8OPylxYh2jY"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lem"
      ],
      "metadata": {
        "id": "rn4YDeaiibxA"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}